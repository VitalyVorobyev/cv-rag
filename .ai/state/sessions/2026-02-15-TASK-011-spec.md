# Task Spec: TASK-011 — Add AnswerService integration tests

**Author**: Architect
**Date**: 2026-02-15
**Priority**: medium
**Effort**: small (1-2 files)

## Goal

Add offline integration-style unit tests for `AnswerService.run()` and `AnswerService.stream()` that validate the end-to-end orchestration: prepare → route → retrieve → generate → citation-validate (→ repair/refuse when needed).

## Background

The CLI and API rely on `cv_rag/answer/service.py` for the full answer flow, but current tests focus on individual helpers (routing/citations) and CLI behavior. A direct `AnswerService` test suite reduces regression risk when changing routing, retrieval wiring, citation enforcement, and streaming behavior.

No external services (Qdrant/GROBID/Ollama/MLX) should be required; tests must be deterministic and offline (monkeypatched / injected fakes only).

## Interface Contract

No production API changes. Tests target these existing public APIs:

```python
from collections.abc import Callable, Generator

from cv_rag.answer.models import AnswerEvent, AnswerRunRequest, AnswerRunResult
from cv_rag.retrieval.hybrid import HybridRetriever
from cv_rag.shared.settings import Settings


class AnswerService:
    def __init__(
        self,
        *,
        retriever: HybridRetriever,
        settings: Settings,
        generate_fn: Callable[..., str],
        stream_generate_fn: Callable[..., Generator[str, None, None]],
    ) -> None:
        ...

    def run(self, request: AnswerRunRequest) -> AnswerRunResult:
        """Return a validated answer (or raise on refusal/validation failure)."""

    def stream(self, request: AnswerRunRequest) -> Generator[AnswerEvent, None, None]:
        """Yield SSE-style events for route, sources, tokens, and final result."""
```

## File Plan

| Action | File | Description |
|--------|------|-------------|
| create | `tests/test_answer_service.py` | Integration-style unit tests for `AnswerService.run()` + `.stream()` with fake retriever + fake generation |

## Error Handling

Test expectations around existing errors/branches:

- `CitationValidationError` (subclass of `CvRagError`) raised by `run()` when citation validation fails after repair and `no_refuse=False`.
- `LookupError` surfaced by `run()` for “not found / no sources / comparison refused” conditions.
- `stream()` emits `AnswerEvent(event="error", data={"message": ...})` for `LookupError` and `ValueError` from `_prepare()`.
- `GenerationError` from streaming path triggers fallback to non-stream `generate_fn` inside `stream()`.

No new errors/settings are needed for this task.

## Testing Notes

Use dependency injection to avoid monkeypatching globals where possible:

- Construct `AnswerService(..., generate_fn=fake_generate, stream_generate_fn=fake_stream_generate)`.
- Provide a fake retriever object implementing:
  - `retrieve(...) -> list[RetrievedChunk]` (used by `retrieve_for_answer`)
  - `_is_irrelevant_result(query, candidates, vector_score_threshold) -> bool` (called by `AnswerService._prepare`)

Recommended test cases (minimum set):

1. **run() happy path (auto + rules routing)**
   - Fake retriever returns prelim chunks (2+ chunks) then final chunks.
   - Fake `generate_fn` returns 4-paragraph answer with valid `[S#]` citations.
   - Assert: `AnswerRunResult.answer`, `sources`, `route.mode`, `citation_valid=True`, no warnings.
2. **run() triggers citation repair loop**
   - First `generate_fn` call returns a draft missing citations; second call returns valid cited answer.
   - Assert: `warnings` includes “Draft failed citation check; attempting repair”, `generate_fn` called twice, and final `citation_valid=True`.
3. **run() raises CitationValidationError when repair fails and refuse enabled**
   - Both draft and repair outputs fail citation validation.
   - Assert: raises `CitationValidationError` and includes `reason` + `draft`.
4. **stream() emits route → sources → token* → done (success streaming)**
   - Fake stream generator yields `["P1...","..."]` chunks that form a valid cited answer.
   - Assert event ordering:
     - first `route`
     - then `sources`
     - then ≥1 `token`
     - final `done` with `citation_valid=True`
5. **stream() falls back when stream generator raises GenerationError**
   - Fake `stream_generate_fn` raises `GenerationError`; fake `generate_fn` returns valid answer.
   - Assert: at least one `token` event yielded (the full text), then `done`.

Optional but useful coverage:

- **stream() citation repair emits `repair` event** when repaired answer becomes valid.
- **_prepare() comparison refusal path**: force `mode="compare"` and have final chunks contain <2 sources for one of the top-2 papers → `stream()` should emit `error` event with “Refusing to answer comparison…”.

Keep tests small and deterministic; do not call real `mlx_lm`, network services, or filesystem outside `tmp_path` for `Settings` paths.

## Out of Scope

- Adding new AnswerService features or changing routing/citation logic
- Testing `HybridRetriever` internals (covered elsewhere)
- End-to-end CLI/API tests (separate concerns)
